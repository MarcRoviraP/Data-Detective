"""
Script para descargar y consolidar datos hist√≥ricos de contaminaci√≥n de Valencia.
Descarga todos los meses desde 1994-04 hasta 2025-11 y crea un CSV consolidado
con solo NO2, O3 y PM10.
"""

import csv
import os
import time
from datetime import datetime
from utils.GetContaminacio import get_historical_data

# Configuraci√≥n
START_YEAR = 1994
START_MONTH = 4
END_YEAR = 2025
END_MONTH = 11

OUTPUT_FILE = "valencia_pollution_consolidated.csv"
POLLUTANTS_TO_KEEP = ['NO2', 'O3', 'PM10']

# Estaciones de Valencia (puedes ajustar seg√∫n necesites)
VALENCIA_STATIONS = [
    'VALENCIA',
    'VLC',
    'PISTA',
    'MOL√ç',
    'POLIT√àCNIC',
    'VIVERS',
    'FRAN√áA',
    'BULEVARD',
    'AVINGUDA',
    'CENTRO'
]


def is_valencia_station(station_name):
    """Verifica si una estaci√≥n pertenece a Valencia."""
    if not station_name:
        return False
    
    station_upper = station_name.upper()
    
    # Buscar palabras clave de Valencia
    for keyword in VALENCIA_STATIONS:
        if keyword in station_upper:
            return True
    
    return False


def download_all_data():
    """Descarga todos los archivos CSV hist√≥ricos."""
    print("=" * 60)
    print("üì• DESCARGANDO DATOS HIST√ìRICOS DE CONTAMINACI√ìN")
    print("=" * 60)
    print(f"Per√≠odo: {START_YEAR}-{START_MONTH:02d} hasta {END_YEAR}-{END_MONTH:02d}")
    print(f"Contaminantes: {', '.join(POLLUTANTS_TO_KEEP)}")
    print("=" * 60)
    
    downloaded_files = []
    failed_downloads = []
    
    current_year = START_YEAR
    current_month = START_MONTH
    
    total_months = (END_YEAR - START_YEAR) * 12 + (END_MONTH - START_MONTH) + 1
    month_count = 0
    
    while current_year < END_YEAR or (current_year == END_YEAR and current_month <= END_MONTH):
        month_count += 1
        print(f"\n[{month_count}/{total_months}] Descargando {current_year}-{current_month:02d}...", end=" ")
        # Comprobar si el fichero ya existe para saltar la descarga
        expected_filename = f"contaminacion_{current_year}_{current_month:02d}.csv"
        if os.path.exists("csv_contaminacion/" + expected_filename):
            print(f"‚úÖ Ya existe")
            downloaded_files.append("csv_contaminacion/" + expected_filename)
            
            # Avanzar al siguiente mes
            current_month += 1
            if current_month > 12:
                current_month = 1
                current_year += 1
            continue
        
        try:
            filepath = get_historical_data(current_year, current_month)
            
            if filepath and os.path.exists(filepath):
                downloaded_files.append(filepath)
                print(f"‚úÖ OK")
            else:
                failed_downloads.append(f"{current_year}-{current_month:02d}")
                print(f"‚ö†Ô∏è No disponible")
        
        except Exception as e:
            failed_downloads.append(f"{current_year}-{current_month:02d}")
            print(f"‚ùå Error: {e}")
        
        # Avanzar al siguiente mes
        current_month += 1
        if current_month > 12:
            current_month = 1
            current_year += 1
        
        # Peque√±a pausa para no saturar el servidor
        time.sleep(0.5)
    
    print("\n" + "=" * 60)
    print(f"‚úÖ Descargados: {len(downloaded_files)} archivos")
    print(f"‚ö†Ô∏è Fallidos: {len(failed_downloads)} archivos")
    
    if failed_downloads:
        print(f"\nMeses sin datos: {', '.join(failed_downloads[:10])}")
        if len(failed_downloads) > 10:
            print(f"... y {len(failed_downloads) - 10} m√°s")
    
    print("=" * 60)
    
    return downloaded_files


# Coordenadas aproximadas de estaciones de Valencia
STATION_COORDINATES = {
    'PISTA': {'lat': 39.4589, 'lon': -0.3768},      # Pista de Silla
    'MOL√ç': {'lat': 39.4891, 'lon': -0.3800},       # Mol√≠ del Sol
    'POLIT√àCNIC': {'lat': 39.4811, 'lon': -0.3400}, # UPV
    'VIVERS': {'lat': 39.4795, 'lon': -0.3667},     # Vivers
    'FRAN√áA': {'lat': 39.4600, 'lon': -0.3500},     # Avd. Francia
    'BULEVARD': {'lat': 39.4470, 'lon': -0.3600},   # Bulevard Sud
    'CENTRO': {'lat': 39.4702, 'lon': -0.3769},     # Centro
    'OLIVERETA': {'lat': 39.4700, 'lon': -0.4000},  # Olivereta
    'PATRAIX': {'lat': 39.4600, 'lon': -0.3900},    # Patraix
    'CABANYAL': {'lat': 39.4700, 'lon': -0.3300},   # Cabanyal
    'LLUCH': {'lat': 39.4700, 'lon': -0.3300},      # Dr. Lluch
    'VLC': {'lat': 39.4700, 'lon': -0.3700},        # Gen√©rico Valencia
    'VALENCIA': {'lat': 39.4700, 'lon': -0.3700},   # Gen√©rico Valencia
}

def get_station_coords(station_name):
    """Obtiene coordenadas para una estaci√≥n."""
    name_upper = station_name.upper()
    
    # B√∫squeda por palabra clave prioritaria
    for key, coords in STATION_COORDINATES.items():
        if key in name_upper:
            return coords
    
    return {'lat': '', 'lon': ''}

def consolidate_data(csv_files):
    """Consolida todos los CSVs en uno solo con filtros aplicados."""
    print("\n" + "=" * 60)
    print("üîÑ CONSOLIDANDO DATOS")
    print("=" * 60)
    
    consolidated_rows = []
    total_rows_processed = 0
    total_rows_kept = 0
    
    # Encabezados del CSV consolidado
    headers = ['COD_ESTACION', 'NOM_ESTACION', 'FECHA', 'NO2', 'O3', 'PM10', 'LATITUD', 'LONGITUD']
    
    for i, filepath in enumerate(csv_files, 1):
        print(f"[{i}/{len(csv_files)}] Procesando {os.path.basename(filepath)}...", end=" ")
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f, delimiter=';')
                file_rows_kept = 0
                
                for row in reader:
                    total_rows_processed += 1
                    
                    # Filtrar solo estaciones de Valencia
                    station_name = row.get('NOM_ESTACION', '').strip()
                    if not is_valencia_station(station_name):
                        continue
                    
                    # Verificar si tiene al menos uno de los contaminantes
                    has_data = False
                    for pollutant in POLLUTANTS_TO_KEEP:
                        value = row.get(pollutant, '').strip()
                        if value and value != '-':
                            has_data = True
                            break
                    
                    if not has_data:
                        continue
                    
                    # Obtener coordenadas
                    coords = get_station_coords(station_name)
                    
                    # Crear fila consolidada
                    consolidated_row = {
                        'COD_ESTACION': row.get('COD_ESTACION', '').strip(),
                        'NOM_ESTACION': station_name,
                        'FECHA': row.get('FECHA', '').strip(),
                        'NO2': row.get('NO2', '-').strip(),
                        'O3': row.get('O3', '-').strip(),
                        'PM10': row.get('PM10', '-').strip(),
                        'LATITUD': coords['lat'],
                        'LONGITUD': coords['lon']
                    }
                    
                    consolidated_rows.append(consolidated_row)
                    file_rows_kept += 1
                    total_rows_kept += 1
                
                print(f"‚úÖ {file_rows_kept} filas")
        
        except Exception as e:
            print(f"‚ùå Error: {e}")
            continue
    
    print("=" * 60)
    print(f"üìä Total procesado: {total_rows_processed:,} filas")
    print(f"‚úÖ Total consolidado: {total_rows_kept:,} filas")
    print(f"üìâ Filtrado: {total_rows_processed - total_rows_kept:,} filas")
    print("=" * 60)
    
    return headers, consolidated_rows


def save_consolidated_csv(headers, rows):
    """Guarda el CSV consolidado."""
    print(f"\nüíæ Guardando archivo consolidado: {OUTPUT_FILE}")
    
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=headers, delimiter=';')
            writer.writeheader()
            writer.writerows(rows)
        
        file_size = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)  # MB
        print(f"‚úÖ Archivo guardado: {OUTPUT_FILE}")
        print(f"üì¶ Tama√±o: {file_size:.2f} MB")
        print(f"üìù Filas: {len(rows):,}")
        
        return True
    
    except Exception as e:
        print(f"‚ùå Error al guardar: {e}")
        return False


def generate_summary(rows):
    """Genera un resumen estad√≠stico del dataset consolidado."""
    print("\n" + "=" * 60)
    print("üìà RESUMEN ESTAD√çSTICO")
    print("=" * 60)
    
    # Contar por a√±o
    years = {}
    stations = set()
    
    for row in rows:
        fecha = row.get('FECHA', '')
        if fecha:
            year = fecha.split('-')[0]
            years[year] = years.get(year, 0) + 1
        
        station = row.get('NOM_ESTACION', '')
        if station:
            stations.add(station)
    
    print(f"\nüìÖ A√±os con datos: {len(years)}")
    print(f"üè¢ Estaciones √∫nicas: {len(stations)}")
    
    print(f"\nüìä Distribuci√≥n por a√±o:")
    for year in sorted(years.keys()):
        count = years[year]
        bar = "‚ñà" * min(50, count // 10)
        print(f"  {year}: {count:>5} filas {bar}")
    
    print(f"\nüè¢ Estaciones de Valencia:")
    for station in sorted(stations):
        print(f"  ‚Ä¢ {station}")
    
    print("=" * 60)


def main():
    """Funci√≥n principal."""
    start_time = time.time()
    
    print("\nüöÄ INICIANDO CONSOLIDACI√ìN DE DATOS HIST√ìRICOS")
    print(f"‚è∞ Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Paso 1: Descargar todos los archivos
    csv_files = download_all_data()
    
    if not csv_files:
        print("\n‚ùå No se descargaron archivos. Abortando.")
        return
    
    # Paso 2: Consolidar datos
    headers, consolidated_rows = consolidate_data(csv_files)
    
    if not consolidated_rows:
        print("\n‚ö†Ô∏è No se encontraron datos de Valencia para consolidar.")
        return
    
    # Paso 3: Guardar CSV consolidado
    if save_consolidated_csv(headers, consolidated_rows):
        # Paso 4: Generar resumen
        generate_summary(consolidated_rows)
    
    # Tiempo total
    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)
    
    print(f"\n‚úÖ PROCESO COMPLETADO")
    print(f"‚è±Ô∏è Tiempo total: {minutes}m {seconds}s")
    print(f"üìÅ Archivo generado: {OUTPUT_FILE}\n")


if __name__ == "__main__":
    main()
